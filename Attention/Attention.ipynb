{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+dVEWpsyohD3w+LuxiwGZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import einops\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "5JFZoDkHpwZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies a causal mask to attention scores, and returns masked scores.\n",
        "Takes an input of size [batch, n_heads, query_pos, key_pos]\n",
        "And outputs a tensor of size [batch, n_heads, query_pos, key_pos]\n",
        "\"\"\"\n",
        "def apply_causal_mask(\n",
        "    attn_scores: Tensor,\n",
        "    masked_value: float = float('-inf')\n",
        ") -> Tensor:\n",
        "    # Define a mask that is True for all positions we want to set probabilities to zero for\n",
        "    mask = torch.triu(torch.ones(attn_scores.shape), diagonal=1).bool()\n",
        "    # Apply the mask to attention scores and replace the masked values with the ignore value masked_value\n",
        "    attn_scores.masked_fill_(mask, masked_value)\n",
        "\n",
        "    return attn_scores\n",
        "\n",
        "\"\"\"\n",
        "Test case for your apply_causal_mask\n",
        "\"\"\"\n",
        "ignore = float('-inf')\n",
        "test1 = apply_causal_mask(torch.tensor([\n",
        "    [1.,2,3],\n",
        "    [4,5,6],\n",
        "    [7,8,9],\n",
        "]), ignore)\n",
        "\n",
        "assert torch.allclose(test1, torch.tensor(\n",
        "    [[1., ignore, ignore],\n",
        "        [4., 5., ignore],\n",
        "        [7., 8., 9.]])), \"Oh no it looks like your matrix doesnt pass test 1\"\n",
        "print(test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCxguqvOpy6b",
        "outputId": "5a5c78bf-8eb0-44da-e121-a1c075b515ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., -inf, -inf],\n",
            "        [4., 5., -inf],\n",
            "        [7., 8., 9.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbUOb_x7sfXs"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, num_heads: Tensor, dim_model: Tensor, dim_head: Tensor) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # hyper parameters\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_model = dim_model\n",
        "        self.dim_head = dim_head\n",
        "\n",
        "        # weights\n",
        "        self.W_Q = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
        "        self.W_K = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
        "        self.W_V = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
        "        self.W_O = nn.Parameter(torch.ones((num_heads, dim_head, dim_model)))\n",
        "\n",
        "        # biases\n",
        "        self.b_Q = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
        "        self.b_K = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
        "        self.b_V = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
        "        self.b_O = nn.Parameter(torch.zeros((dim_model)))\n",
        "\n",
        "    def init_testing_weights(self):\n",
        "        with torch.no_grad():\n",
        "            self.W_Q.normal_()\n",
        "            self.W_K.normal_()\n",
        "            self.W_V.normal_()\n",
        "            self.W_O.normal_()\n",
        "            self.b_Q.normal_()\n",
        "            self.b_K.normal_()\n",
        "            self.b_V.normal_()\n",
        "            self.b_O.normal_()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Forward pass of the attention layer.\n",
        "    Takes a tensor of shape [batch, tokens, dim_model]\n",
        "    Outputs a tensor of shape [batch, tokens, dim_model]\n",
        "    \"\"\"\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        batch_size = x.shape[0]\n",
        "        tokens_size = x.shape[1]\n",
        "\n",
        "        # Calculate query, key and value vectors\n",
        "        q = einops.einsum(\n",
        "            x, self.W_Q,\n",
        "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
        "        ) + self.b_Q\n",
        "        assert q.shape == torch.Size([batch_size, tokens_size, self.num_heads, self.dim_head])\n",
        "        k = einops.einsum(\n",
        "            x, self.W_K,\n",
        "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
        "        ) + self.b_K\n",
        "        assert k.shape == torch.Size([batch_size, tokens_size, self.num_heads, self.dim_head])\n",
        "        v = einops.einsum(\n",
        "            x, self.W_V,\n",
        "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
        "        ) + self.b_V\n",
        "        assert v.shape == torch.Size([batch_size, tokens_size, self.num_heads, self.dim_head])\n",
        "\n",
        "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
        "        attn_scores = einops.einsum(\n",
        "            q, k,\n",
        "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\",\n",
        "        )\n",
        "        assert attn_scores.shape == torch.Size([batch_size, self.num_heads, tokens_size, tokens_size])\n",
        "        attn_scores_masked = apply_causal_mask(attn_scores, float('-inf'))\n",
        "        attn_probs = torch.softmax(attn_scores_masked / self.dim_head**0.5, dim=-1)\n",
        "\n",
        "        # Take weighted sum of value vectors, according to attention probabilities\n",
        "        z = einops.einsum(\n",
        "            v, attn_probs,\n",
        "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
        "        )\n",
        "        assert z.shape == torch.Size([batch_size, tokens_size, self.num_heads, self.dim_head])\n",
        "\n",
        "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
        "        attn_out = einops.einsum(\n",
        "            z, self.W_O,\n",
        "            \"batch posn nheads d_head, nheads d_head d_model -> batch posn d_model\",\n",
        "        ) + self.b_O\n",
        "        assert attn_out.shape == torch.Size([batch_size, tokens_size, self.dim_model])\n",
        "\n",
        "        return attn_out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code against solution, no need to change this\n",
        "\n",
        "batch_size = 12\n",
        "tokens_dim = 20\n",
        "dim_model = 30\n",
        "dim_heads = 10\n",
        "num_heads = 2\n",
        "ground_truth = Attention(num_heads, dim_model, dim_heads)\n",
        "user_model = Attention(num_heads, dim_model, dim_heads)\n",
        "test = torch.rand((batch_size, tokens_dim, dim_model))\n",
        "\n",
        "truth_output = ground_truth(test)\n",
        "user_output = user_model(test)\n",
        "\n",
        "assert torch.allclose(truth_output, user_output), \"Uh oh your model doesn't give the same outputs\"\n",
        "print(\"passed all tests!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPqaBgycp4w2",
        "outputId": "3831a037-339f-4cf3-b779-8334f15d4ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed all tests!\n"
          ]
        }
      ]
    }
  ]
}