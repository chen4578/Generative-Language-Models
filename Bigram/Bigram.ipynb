{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWj1N5gv6Nd3WbS+EkkaYk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Run this block\n",
        "import torch\n",
        "from typing import List\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests"
      ],
      "metadata": {
        "id": "O9rYOdZTS96u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = \"\"\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ., '\\\"\"()[]!?\"\"\"\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return [char for char in text if char in vocab]\n",
        "\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
        "\n",
        "def vectorize(tokens: List[str]) -> torch.Tensor:\n",
        "    indices = torch.tensor([char_to_index[char] for char in tokens])\n",
        "    return F.one_hot(indices, num_classes=len(vocab)).float()\n",
        "\n",
        "def detokenize(tensor: torch.Tensor):\n",
        "    indices = tensor.argmax(dim=-1).tolist()\n",
        "    return ''.join(index_to_char[idx] for idx in indices)\n",
        "\n",
        "class EmbeddingProjection(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.projection(x)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.tokens = tokenize(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = self.tokens[idx:idx+self.seq_length]\n",
        "        target_seq = self.tokens[idx+1:idx+self.seq_length+1]\n",
        "        return vectorize(input_seq).squeeze(), vectorize(target_seq).squeeze()"
      ],
      "metadata": {
        "id": "2Sl0p-39TDIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download toy data (Shakespeare sonnets)\n",
        "url = \"https://www.gutenberg.org/files/1041/1041-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text.split(\"THE SONNETS\", 1)[1].split(\"End of the Project Gutenberg EBook\", 1)[0]\n",
        "\n",
        "# Prepare the dataset\n",
        "seq_length = 1\n",
        "dataset = TextDataset(text, seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "batch, target = next(iter(dataloader))\n",
        "# batch is the input tensor to your model, shape (batch_size, vocab_size)\n",
        "# It's the vector representation of the single token your bigram model has as context.\n",
        "# target is the target tensor, shape (batch_size, vocab_size), representing the next token in the sequence (which your model is tasked with predicting).\n",
        "print(batch.shape, target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ZHIb5iTKF0",
        "outputId": "62f0dccb-e3a7-4d97-de6b-c2f473df02ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64]) torch.Size([32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detokenized_targets = detokenize(target)\n",
        "for index, item in enumerate(detokenize(batch[:6])):\n",
        "    print(f\"Context: {item}, Target: {detokenized_targets[index]}\")\n",
        "\n",
        "# Seems like a tough task, eh?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRY2oksyTN-Z",
        "outputId": "02998bef-9e0f-4eaa-f329-8db986d0dc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context:  , Target: m\n",
            "Context: g, Target:  \n",
            "Context: a, Target: v\n",
            "Context:  , Target: t\n",
            "Context:  , Target: t\n",
            "Context: ,, Target:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1:\n",
        "# Implement a multilayer linear model. Feel free to use nn.Linear and nn.ReLU.\n",
        "\n",
        "# Your projection layer is a linear projection from vocab size -> model size. Make sure your intermediate linear layers are projections from model size -> model size,\n",
        "# and your final layer is a projection from model size -> vocab size.\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, model_dim = 128, vocab_size = len(vocab)):\n",
        "        super().__init__()\n",
        "        self.projection = EmbeddingProjection(vocab_size, model_dim)\n",
        "        self.layer1 = nn.Linear(model_dim, model_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(model_dim, model_dim)\n",
        "        self.layer3 = nn.Linear(model_dim, vocab_size)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Our model takes in a tensor of shape (batch_size, vocab_size) and returns a tensor of shape (batch_size, vocab_size).\n",
        "    # Don't forget to embed the input tensor before passing it through the linear layers.\n",
        "        x = self.projection(x)\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "def test_bigram_model():\n",
        "    model = BigramModel()\n",
        "    out = model(batch)\n",
        "    assert out.shape == target.shape, f\"Expected output shape {target.shape} but got {out.shape}\"\n",
        "    print(\"Success!\")\n",
        "\n",
        "test_bigram_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdlA3cs6TVft",
        "outputId": "2d7dfa15-2adc-4de3-951f-eeb484274189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm, trange\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# TODO: Initialize model and transfer it to the device\n",
        "# TODO: Initialize optimizer (from torch.optim). We recommend using AdamW with the default parameters.\n",
        "# TODO: Initialize the loss criterion (from torch.nn). Since this is basically a classification task (we decide which character comes next), we recommend using nn.CrossEntropyLoss.\n",
        "\n",
        "model = BigramModel().to(device)\n",
        "optimizer = optim.AdamW(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "loss_ema = None\n",
        "for epoch in range(num_epochs):\n",
        "    with tqdm(dataloader) as pbar:\n",
        "        for batch, target in pbar:\n",
        "            # TODO: Training loop\n",
        "            # ------------------\n",
        "            batch, target = batch.to(device), target.to(device)\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            # ------------------\n",
        "            if loss_ema is None:\n",
        "                loss_ema = loss.item()\n",
        "            else:\n",
        "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
        "            pbar.set_description(f\"Loss: {round(loss.item(), 3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_k4A1-eTaTB",
        "outputId": "0f1c2c02-b28c-48bc-b60f-324b1de8c66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 1.993: 100%|██████████| 2888/2888 [00:20<00:00, 139.77it/s]\n",
            "Loss: 2.305: 100%|██████████| 2888/2888 [00:21<00:00, 136.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rplBeZsvqufk",
        "outputId": "cfa39a84-eecd-4c6c-f224-35e935278a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2352524896.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_seq = torch.tensor(vectorize(tokenize(start_text))).unsqueeze(0).to(device)[:, -1, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "Shall I compare thee to a summer's day? the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "model.eval()\n",
        "start_text = \"Shall I compare thee to a summer's day?\"\n",
        "input_seq = torch.tensor(vectorize(tokenize(start_text))).unsqueeze(0).to(device)[:, -1, :]\n",
        "generated_text = start_text\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        output = model(input_seq)\n",
        "        next_char = output.argmax(dim=-1)\n",
        "        generated_text += index_to_char[next_char.item()]\n",
        "        input_seq = F.one_hot(next_char, num_classes=len(vocab)).float()\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)"
      ]
    }
  ]
}