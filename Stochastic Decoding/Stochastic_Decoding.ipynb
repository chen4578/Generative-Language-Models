{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWb/gqq3gg4T8TbBh5v0yh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import types, sys, importlib.util\n",
        "\n",
        "imp = types.SimpleNamespace(\n",
        "    reload=importlib.reload,\n",
        "    find_module=lambda name, path=None: importlib.util.find_spec(name)\n",
        ")\n",
        "sys.modules['imp'] = imp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    else:\n",
        "        return \"cpu\"\n",
        "\n",
        "\n",
        "device = torch.device(get_device())\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "gno7Mu3by7x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "zeP8AlVKy94S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_n_tokens(\n",
        "    input_ids: torch.Tensor, n: int, sampling_function: callable\n",
        ") -> torch.Tensor:\n",
        "    generated = input_ids.clone()\n",
        "    for _ in range(n):\n",
        "        with torch.no_grad():\n",
        "            logits = model(generated).logits[:, -1, :]\n",
        "        next_token = sampling_function(logits)\n",
        "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
        "    return generated\n",
        "\n",
        "\n",
        "def sample_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Takes logits and converts them to probabilities and samples from thier distribution\n",
        "    \"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, num_samples=1).squeeze(-1)"
      ],
      "metadata": {
        "id": "msSNecURzFpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample vocabulary\n",
        "sample_vocab = [\n",
        "    \"token1\",\n",
        "    \"token2\",\n",
        "    \"token3\",\n",
        "    \"token4\",\n",
        "    \"token5\",\n",
        "    \"token6\",\n",
        "    \"token7\",\n",
        "    \"token8\",\n",
        "    \"token9\",\n",
        "    \"token10\",\n",
        "]\n",
        "vocabulary_size = len(sample_vocab)\n",
        "\n",
        "# Sample logits\n",
        "sample_logits = torch.tensor(\n",
        "    [\n",
        "        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
        "        [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0],\n",
        "        [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],\n",
        "        [1.0, 1.0, 1.0, 1.0, 10.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "nOu1FzQGzIvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert token indices to vocabulary tokens\n",
        "def indices_to_tokens(indices):\n",
        "    return [sample_vocab[i] for i in indices]\n",
        "\n",
        "def greedy_search(logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Select the token with the largest logit\n",
        "    \"\"\"\n",
        "    #### Your code here ####\n",
        "    return torch.argmax(logits, dim=1)\n",
        "    ########################\n",
        "\n",
        "# Test greedy search\n",
        "greedy_results = greedy_search(sample_logits)\n",
        "print(\"Greedy Search Results:\", indices_to_tokens(greedy_results))"
      ],
      "metadata": {
        "id": "fb2DZqIVOdcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits: torch.Tensor, k: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns new logits with all values, except for the k largest, set to -inf\n",
        "    \"\"\"\n",
        "    assert k >= 1, f\"k was set to {k}, k must be positive\"\n",
        "\n",
        "    #### Your code here ####\n",
        "\n",
        "    top_k_indices = torch.topk(logits, k).indices\n",
        "\n",
        "    # Create a mask with -inf for all values except the top k\n",
        "    mask = torch.ones_like(logits) * float('-inf')\n",
        "\n",
        "    # Fill the mask with the original logits values at the top k indices\n",
        "    # This effectively leaves only the top k logits unchanged, while others are set to -inf\n",
        "    # scatter is used to assign values at specific indices in the mask tensor\n",
        "    mask.scatter_(-1, top_k_indices, logits)\n",
        "\n",
        "    return mask\n",
        "    pass"
      ],
      "metadata": {
        "id": "D31DJx5bzNi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test top-k sampling\n",
        "k = 1\n",
        "top_k_logits = top_k_sampling(sample_logits, k)\n",
        "top_k_results = sample_from_logits(top_k_logits)\n",
        "print(f\"Top-{k} Sampling Results:\", indices_to_tokens(top_k_results))\n",
        "k = 3\n",
        "top_k_logits = top_k_sampling(sample_logits, k)\n",
        "top_k_results = sample_from_logits(top_k_logits)\n",
        "print(f\"Top-{k} Sampling Results:\", indices_to_tokens(top_k_results))"
      ],
      "metadata": {
        "id": "6mfyKjHazyj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(logits: torch.Tensor, p: float):\n",
        "    \"\"\"\n",
        "    Perform top-p (nucleus) sampling on logits.\n",
        "\n",
        "    Args:\n",
        "    logits: torch.Tensor of shape (..., vocab_size)\n",
        "    p: float, cumulative probability threshold\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor of the same shape as logits, with values outside the top-p set to -inf\n",
        "    \"\"\"\n",
        "    #### Your code here ####\n",
        "    # calculate the probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # sort them\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    # calculate the cumalative probabilities\n",
        "    cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    # Remove tokens with cumulative probability above the threshold\n",
        "    #over_threshold_index = torch.where(cumsum > p)\n",
        "    over_threshold_indices = cumsum > p\n",
        "    over_threshold_indices[:,0] = False\n",
        "    mask = torch.full_like(logits, float('-inf'))  # Initialize a mask with -inf\n",
        "\n",
        "    # Scatter back the valid logits where cumulative sum is <= p\n",
        "    mask.scatter_(-1, sorted_indices, sorted_probs)\n",
        "\n",
        "    # Apply the mask to the logits, setting low-probability tokens to -inf\n",
        "    logits = logits + mask  # This will apply the mask to logits (set logits outside the top-p to -inf)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "_sYjD4xez_Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test top-p sampling\n",
        "p = 0.05\n",
        "top_p_logits = top_p_sampling(sample_logits, p)\n",
        "top_p_results = sample_from_logits(top_p_logits)\n",
        "print(f\"Top-p Sampling Results (p={p}):\", indices_to_tokens(top_p_results))\n",
        "p = 0.9\n",
        "top_p_logits = top_p_sampling(sample_logits, p)\n",
        "top_p_results = sample_from_logits(top_p_logits)\n",
        "print(f\"Top-p Sampling Results (p={p}):\", indices_to_tokens(top_p_results))"
      ],
      "metadata": {
        "id": "yXtcPBOzz1Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temperature_sampling(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Scales logits by temprature\n",
        "    \"\"\"\n",
        "    #### Your code here ####\n",
        "    # should be one line of code\n",
        "    return logits / temperature\n",
        "    ########################\n",
        "    pass"
      ],
      "metadata": {
        "id": "JMJ5a7XV0Q3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test temperature sampling\n",
        "temperature = 0.1\n",
        "temp_logits = temperature_sampling(sample_logits, temperature)\n",
        "temp_results = sample_from_logits(temp_logits)\n",
        "print(\n",
        "    f\"Temperature Sampling Results (T={temperature}):\", indices_to_tokens(temp_results)\n",
        ")\n",
        "temperature = 5\n",
        "temp_logits = temperature_sampling(sample_logits, temperature)\n",
        "temp_results = sample_from_logits(temp_logits)\n",
        "print(\n",
        "    f\"Temperature Sampling Results (T={temperature}):\", indices_to_tokens(temp_results)\n",
        ")"
      ],
      "metadata": {
        "id": "7S6YLnwkljkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate n tokens using different sampling strategies\n",
        "n_tokens = 40\n",
        "\n",
        "# Prepare input\n",
        "text = \"Once upon a time, there was a\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "greedy_output = generate_n_tokens(input_ids, n_tokens, greedy_search)\n",
        "top_k_output = generate_n_tokens(\n",
        "    input_ids, n_tokens, lambda x: sample_from_logits(top_k_sampling(x, k=5))\n",
        ")\n",
        "top_p_output = generate_n_tokens(\n",
        "    input_ids, n_tokens, lambda x: sample_from_logits(top_p_sampling(x, p=0.05))\n",
        ")\n",
        "temp_output = generate_n_tokens(\n",
        "    input_ids,\n",
        "    n_tokens,\n",
        "    lambda x: sample_from_logits(temperature_sampling(x, temperature=1.5)),\n",
        ")\n",
        "\n",
        "# Decode outputs\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], clean_up_tokenization_spaces=True))\n",
        "print(\"Top-k:\", tokenizer.decode(top_k_output[0], clean_up_tokenization_spaces=True))\n",
        "print(\"Top-p:\", tokenizer.decode(top_p_output[0], clean_up_tokenization_spaces=True))\n",
        "print(\n",
        "    \"Temperature:\", tokenizer.decode(temp_output[0], clean_up_tokenization_spaces=True)\n",
        ")\n",
        "\n",
        "# often times you will see temprature and top p or top k combined so that we remove all unlikely next tokens and\n",
        "# make some of the somewhat likely tokens more likely to be sampled\n",
        "# try playing around with the temprature and p and k and see how good of an output you can get!\n",
        "\n",
        "# Generate n tokens using different sampling strategies\n",
        "n_tokens = 40\n",
        "\n",
        "# Prepare input\n",
        "text = \"Once upon a time, there was a\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "p = 0.8\n",
        "k = 20\n",
        "temperature = .15\n",
        "\n",
        "\n",
        "def temp_top_k(x):\n",
        "    return sample_from_logits(\n",
        "        temperature_sampling(top_k_sampling(x, k=k), temperature=temperature)\n",
        "    )\n",
        "\n",
        "\n",
        "def temp_top_p(x):\n",
        "    return sample_from_logits(\n",
        "        temperature_sampling(top_p_sampling(x, p=p), temperature=temperature)\n",
        "    )\n",
        "\n",
        "\n",
        "temp_top_p_output = generate_n_tokens(input_ids, n_tokens, temp_top_p)\n",
        "temp_top_k_output = generate_n_tokens(input_ids, n_tokens, temp_top_k)\n",
        "\n",
        "# Decode outputs\n",
        "print(\n",
        "    \"Temperature and Top-k:\",\n",
        "    tokenizer.decode(temp_top_k_output[0], clean_up_tokenization_spaces=True),\n",
        ")\n",
        "print(\n",
        "    \"Temperature and Top-p:\",\n",
        "    tokenizer.decode(temp_top_p_output[0], clean_up_tokenization_spaces=True),\n",
        ")"
      ],
      "metadata": {
        "id": "BeTFpO33lkEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}